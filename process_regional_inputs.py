# generate inputs for the forage model on regional properties, Laikipia

# import arcpy
import pandas as pd
import numpy as np
import os
import pygeoprocessing.geoprocessing
from osgeo import gdal
import tempfile
from tempfile import mkstemp
import shutil

# arcpy.CheckOutExtension("Spatial")

def calculate_zonal_averages(raster_list, zonal_shp, save_as):
    """Calculate averages of the rasters in raster_list within zones
    identified by the zonal_shp.  Store averages in table with a row for
    each zone, identified by the file path save_as."""
    
    tempdir = tempfile.mkdtemp()
    zonal_raster = os.path.join(tempdir, 'zonal_raster.tif')
    field = "FID"
    arcpy.FeatureToRaster_conversion(zonal_shp, field, zonal_raster)
    
    outdir = tempfile.mkdtemp()
    arcpy.BuildRasterAttributeTable_management(zonal_raster)
    for raster in raster_list:
        intermediate_table = os.path.join(outdir, os.path.basename(raster)[:-4]
                                          + '.dbf')
        arcpy.sa.ZonalStatisticsAsTable(zonal_raster, 'VALUE', raster,
                                        intermediate_table,
                                        statistics_type="MEAN")
    sum_dict = {}
    arcpy.env.workspace = outdir
    tables = arcpy.ListTables()
    for table in tables:
        sum_dict[table[:-4]] = []
        sum_dict['zone_' + table[:-4]] = []
        fields = arcpy.ListFields(table)
        field_names = ['Value', 'MEAN']
        with arcpy.da.SearchCursor(table, field_names) as cursor:
            try:
                for row in cursor:
                    sum_dict['zone_' + table[:-4]].append(row[0])
                    sum_dict[table[:-4]].append(row[1])
            except:
                import pdb; pdb.set_trace()
                print table
    import pdb; pdb.set_trace()
    sum_df = pd.DataFrame.from_dict(sum_dict)
    remove_cols = [f for f in sum_df.columns.values if f.startswith('zone')]
    sum_df['zone'] = sum_df[[-1]]
    sum_df = sum_df.drop(remove_cols, axis=1)
    sum_df.to_csv(save_as, index=False)
    
    try:
        shutil.rmtree(tempdir)
        shutil.rmtree(outdir)
    except:
        print "Warning, temp files cannot be deleted"
        pass

def calc_soil_table(zonal_shp, save_as):
    """Make the table containing soil inputs for each property."""
    
    arcpy.env.overwriteOutput = 1
    
    soil_dir = r"C:\Users\Ginger\Documents\NatCap\GIS_local\Kenya_forage\Laikipia_soil_250m\averaged"
    raster_list = [os.path.join(soil_dir, f) for f in os.listdir(soil_dir)]
    calculate_zonal_averages(raster_list, zonal_shp, save_as)

def join_site_lat_long(zonal_shp, soil_table):
    """Calculate latitude and longitude of the centroid of each property and
    join it to the soil table to be used as input for site.100 file."""
    
    soil_df = pd.read_csv(soil_table).set_index("zone")
    soil_df["latitude"] = "NA"
    soil_df["longitude"] = "NA"
    arcpy.env.outputCoordinateSystem = arcpy.SpatialReference(4326)  # WGS 1984
    tempdir = tempfile.mkdtemp()
    point_shp = os.path.join(tempdir, 'centroid.shp')
    arcpy.FeatureToPoint_management(zonal_shp, point_shp, "CENTROID")
    arcpy.AddXY_management(point_shp)
    with arcpy.da.SearchCursor(point_shp, ['FID', 'POINT_X', 'POINT_Y']) as \
                               cursor:
        for row in cursor:
            soil_df = soil_df.set_value([row[0]], 'longitude', row[1])
            soil_df = soil_df.set_value([row[0]], 'latitude', row[2])
    soil_df.to_csv(soil_table)
    
    try:
        shutil.rmtree(tempdir)
    except:
        print "Warning, temp files cannot be deleted"
        pass
    
def write_site_files(template, soil_table, save_dir):
    """Write the site.100 file for each property, using the "zone" field in the
    soil table as identifier for each property."""
    
    in_list = pd.read_csv(soil_table).to_dict(orient="records")
    for inputs_dict in in_list:
        fh, abs_path = mkstemp()
        os.close(fh)
        with open(abs_path, 'wb') as newfile:
            first_line = '%s (generated by script)\r\n' % inputs_dict['zone']
            newfile.write(first_line)
            with open(template, 'rb') as old_file:
                next(old_file)
                for line in old_file:
                    if '  \'SITLAT' in line:
                        item = '{:0.12f}'.format(inputs_dict['latitude'])[:7]
                        newline = '%s           \'SITLAT\'\r\n' % item
                    elif '  \'SITLNG' in line:
                        item = '{:0.12f}'.format(inputs_dict['longitude'])[:7]
                        newline = '%s           \'SITLNG\'\r\n' % item
                    elif '  \'SAND' in line:
                        num = inputs_dict['geonode-sndppt_m_sl_0-15'] / 100.0
                        item = '{:0.12f}'.format(num)[:7]
                        newline = '%s           \'SAND\'\r\n' % item
                    elif '  \'SILT' in line:
                        num = inputs_dict['geonode-sltppt_m_sl_0-15'] / 100.0
                        item = '{:0.12f}'.format(num)[:7]
                        newline = '%s           \'SILT\'\r\n' % item
                    elif '  \'CLAY' in line:
                        num = inputs_dict['geonode-clyppt_m_sl_0-15'] / 100.0
                        item = '{:0.12f}'.format(num)[:7]
                        newline = '%s           \'CLAY\'\r\n' % item
                    elif '  \'BULKD' in line:
                        item = '{:0.12f}'.format(inputs_dict[
                                       'geonode-bldfie_m_sl_0-15_div1000'])[:7]
                        newline = '%s           \'BULKD\'\r\n' % item
                    elif '  \'PH' in line:
                        item = '{:0.12f}'.format(inputs_dict[
                                       'geonode-phihox_m_sl_0-15_div10'])[:7]
                        newline = '%s           \'PH\'\r\n' % item
                    else:
                        newline = line
                    newfile.write(newline)
        save_as = os.path.join(save_dir, '{}.100'.format(
                                                     int(inputs_dict['zone'])))
        shutil.copyfile(abs_path, save_as)
        os.remove(abs_path)
        # generate weather statistics (manually :( )

def make_sch_files(template_hist, template_extend, soil_table, save_dir):
    """Write the schedule files (hist and extend) to run each site, using the
    "zone" field in the soil table as identifier and name for each property."""
    
    def copy_sch_file(template, site_name, weather_file, save_as):
        fh, abs_path = mkstemp()
        os.close(fh)
        with open(abs_path, 'wb') as newfile:
            with open(template, 'rb') as old_file:
                for line in old_file:
                    if '  Site file name' in line:
                        item = '{:14}'.format('{}.100'.format(site_name))
                        newline = '{}Site file name\r\n'.format(item)
                    elif '.wth' in line:
                        newline = '{}\r\n'.format(weather_file)
                    else:
                        newline = line
                    newfile.write(newline)
        shutil.copyfile(abs_path, save_as)
        os.remove(abs_path)

    site_df = pd.read_csv(soil_table)
    for site_name in site_df.zone:
        weather_file = '{}.wth'.format(site_name)
        save_as = os.path.join(save_dir, '{}_hist.sch'.format(site_name))
        copy_sch_file(template_hist, site_name, weather_file, save_as)
        save_as = os.path.join(save_dir, '{}.sch'.format(site_name))
        copy_sch_file(template_extend, site_name, weather_file, save_as)

def clip_FEWS_files(FEWS_folder, clipped_folder, aoi_shp):
    """Clip the raw FEWS files to an aoi to speed up later processing."""
    
    bil_files = [f for f in os.listdir(FEWS_folder) if f.endswith(".bil")]
    raster_nodata = 9999
    out_pixel_size = pygeoprocessing.geoprocessing.get_cell_size_from_uri(
                                       os.path.join(FEWS_folder, bil_files[0]))
    for b in bil_files:
        bil = os.path.join(FEWS_folder, b)
        # arcpy.DefineProjection_management(bil, 102022)  # Africa Albers eq. area conic
        clipped_raster_uri = os.path.join(clipped_folder, b)
        pygeoprocessing.geoprocessing.vectorize_datasets(
            [bil], lambda x: x, clipped_raster_uri, gdal.GDT_Float64,
            raster_nodata, out_pixel_size, "union",
            dataset_to_align_index=0, aoi_uri=aoi_shp,
            assert_datasets_projected=False, vectorize_op=False)

def clim_tables_to_inputs(prec_table, temp_table, input_folder):
    """Convert tables with monthly precipitation values (generated by the 
    function process_FEWS_files) and a table with monthly temp values
    generated with process_worldclim to input files for CENTURY."""
    
    temp_df = pd.read_csv(temp_table)
    prec_df = pd.read_csv(prec_table)
    if 'year' in prec_df.columns.values:
        year_list = [2000 + y for y in prec_df.year.unique()][1:]
        year_list = [int(y) for y in year_list]
    else:
        year_list = [2015]
    for site in prec_df.site.unique():
        sub_p_df = prec_df.loc[(prec_df["site"] == site) & (
                               prec_df["month"].notnull())]
        sub_t_df = temp_df.loc[(temp_df["site"] == site)]
        sub_t_df = sub_t_df.sort_values(by='month')
        # calc average prec in month 1
        avg_mo1 = sub_p_df.loc[(sub_p_df["month"] == 1),
                               "prec"].values.mean() / 10.0
        trans_dict = {'label': ['prec'] * len(year_list),
                      'year': year_list * 3}
        for mon in range(1, 13):
            p_vals = sub_p_df.loc[(sub_p_df["month"] == mon),
                                  "prec"].values.tolist()
            trans_dict[mon] = [v / 10.0 for v in p_vals]  # RFE vals in mm
            tmin = sub_t_df.loc[(sub_t_df['month'] == mon), 'tmin'].values
            tmax = sub_t_df.loc[(sub_t_df['month'] == mon), 'tmax'].values
            trans_dict[mon].extend([tmin] * len(year_list))
            trans_dict[mon].extend([tmax] * len(year_list))
        if len(trans_dict[1]) < len(trans_dict[2]):  # RFE missing Jan-00 val
            trans_dict[1].insert(0, avg_mo1)
        trans_dict['label'].extend(['tmin'] * len(year_list))
        trans_dict['label'].extend(['tmax'] * len(year_list))
        df = pd.DataFrame(trans_dict)
        cols = df.columns.tolist()
        cols = cols[-2:-1] + cols[-1:] + cols[:-2]
        df = df[cols]
        df['sort_col'] = df['year']
        df.loc[(df['label'] == 'prec'), 'sort_col'] = df.sort_col + 0.1
        df.loc[(df['label'] == 'tmin'), 'sort_col'] = df.sort_col + 0.2
        df.loc[(df['label'] == 'tmax'), 'sort_col'] = df.sort_col + 0.3
        df = df.sort_values(by='sort_col')
        df = df.drop('sort_col', 1)
        save_as = os.path.join(input_folder, '{}.wth'.format(site))
        formats = ['%4s', '%6s'] + ['%7.2f'] * 12
        np.savetxt(save_as, df.values, fmt=formats, delimiter='')

def remove_wth_from_sch(input_dir):
    """To run the simulation with worldclim precipitation, must remove the
    reference to empirical weather and use just the averages in the site.100
    file."""
    
    sch_files = [f for f in os.listdir(input_dir) if f.endswith('.sch')]
    sch_files = [f for f in sch_files if not f.endswith('hist.sch')]
    sch_files = [os.path.join(input_dir, f) for f in sch_files]
    for sch in sch_files:
        fh, abs_path = mkstemp()
        os.close(fh)
        with open(abs_path, 'wb') as newfile:
            with open(sch, 'rb') as old_file:
                for line in old_file:
                    if '.wth' in line:
                        line = old_file.next()
                    if "Weather choice" in line:
                        newline = "M             Weather choice\r\n"
                        newfile.write(newline)
                    else:
                        newfile.write(line)
        shutil.copyfile(abs_path, sch)
        os.remove(abs_path)
    
def process_worldclim_temp(worldclim_folder, zonal_shp, save_as):
    """Make a table of max and min monthly temperature for points which are the
    centroids of properties (features in zonal_shp), from worldclim rasters."""
    
    tempdir = tempfile.mkdtemp()
    
    # make property centroid shapefile to extract values to points
    point_shp = os.path.join(tempdir, 'centroid.shp')
    arcpy.FeatureToPoint_management(zonal_shp, point_shp, "CENTROID")
    
    # extract monthly values to each point
    rasters = [f for f in os.listdir(worldclim_folder) if f.endswith('.tif')]
    field_list = [r[:5] if r[5] == '_' else r[:6] for r in rasters]
    raster_files = [os.path.join(worldclim_folder, f) for f in rasters]
    ex_list = zip(raster_files, field_list)
    arcpy.sa.ExtractMultiValuesToPoints(point_shp, ex_list)
    
    # read from shapefile to newly formatted table
    field_list.insert(0, 'FID')
    month_list = [10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # totally lazy hack
    temp_dict = {'site': [], 'month': [], 'tmin': [], 'tmax': []}
    with arcpy.da.SearchCursor(point_shp, field_list) as cursor:
        for row in cursor:
            site = row[0]
            temp_dict['site'].extend([site] * 12)
            for f_idx in range(1, len(field_list)):
                field = field_list[f_idx]
                month = field[4:6]
                if field.startswith('tmin'):
                    temp_dict['tmin'].append(row[f_idx] / 10.0)
                elif field.startswith('tmax'):
                    temp_dict['tmax'].append(row[f_idx] / 10.0)
                else:
                    raise Exception, "value not recognized"
            temp_dict['month'].extend(month_list)
    temp_df = pd.DataFrame.from_dict(temp_dict)
    temp_df.to_csv(save_as, index=False)

def process_worldclim_precip(worldclim_folder, zonal_shp, save_as):
    """Make a table of monthly precip for points which are the
    centroids of properties (features in zonal_shp), from worldclim rasters."""
    
    tempdir = tempfile.mkdtemp()
    
    # make property centroid shapefile to extract values to points
    point_shp = os.path.join(tempdir, 'centroid.shp')
    arcpy.FeatureToPoint_management(zonal_shp, point_shp, "CENTROID")
    
    # extract monthly values to each point
    rasters = [f for f in os.listdir(worldclim_folder) if f.endswith('.tif')]
    field_list = [r[:5] if r[5] == '_' else r[:6] for r in rasters]
    raster_files = [os.path.join(worldclim_folder, f) for f in rasters]
    ex_list = zip(raster_files, field_list)
    arcpy.sa.ExtractMultiValuesToPoints(point_shp, ex_list)
    
    # read from shapefile to newly formatted table
    field_list.insert(0, 'FID')
    month_list = [10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # totally lazy hack
    prec_dict = {'site': [], 'month': [], 'prec': []}
    with arcpy.da.SearchCursor(point_shp, field_list) as cursor:
        for row in cursor:
            site = row[0]
            prec_dict['site'].extend([site] * 12)
            for f_idx in range(1, len(field_list)):
                field = field_list[f_idx]
                month = field[4:6]
                prec_dict['prec'].append(row[f_idx])
            prec_dict['month'].extend(month_list)
    prec_df = pd.DataFrame.from_dict(prec_dict)
    prec_df.to_csv(save_as, index=False)    

def process_FEWS_files(FEWS_folder, zonal_shp, prec_table):
    """Calculate precipitation at property centroids from FEWS RFE (rainfall
    estimate) rasters.  The zonal_shp shapefile should be properties."""
    
    # the files give dekadal (10-day) estimates, with the filename format
    # 'ea15011.bil' for the 1st period of the first month of year 2015,
    # 'ea08121.bil' for the 1st period of the 12th month of year 2008, etc
    
    # tempdir = tempfile.mkdtemp() todo remove
    tempdir = r"C:\Users\Ginger\Documents\NatCap\GIS_local\Kenya_forage\FEWS_RFE_sum"
    
    # make property centroid shapefile to extract values to points
    point_shp = os.path.join(tempdir, 'centroid.shp')
    arcpy.FeatureToPoint_management(zonal_shp, point_shp, "CENTROID")
    
    def sum_rasters(raster_list, save_as, cell_size):
        def sum_op(*rasters):
            return np.sum(np.array(rasters), axis=0)
        nodata = 9999
        pygeoprocessing.geoprocessing.vectorize_datasets(
                raster_list, sum_op, save_as,
                gdal.GDT_UInt16, nodata, cell_size, "union",
                dataset_to_align_index=0, assert_datasets_projected=False,
                vectorize_op=False, datasets_are_pre_aligned=True)
        
    bil_files = [f for f in os.listdir(FEWS_folder) if f.endswith(".bil")]
    
    # set nodata value
    for f in bil_files:
        raster = os.path.join(FEWS_folder, f)
        source_ds = gdal.Open(raster)
        band = source_ds.GetRasterBand(1)
        band.SetNoDataValue(9999)
        source_ds = None
    template = raster = os.path.join(FEWS_folder, bil_files[0])
    cell_size = pygeoprocessing.geoprocessing.get_cell_size_from_uri(template)
    
    # calculate monthly values from dekadal (10-day) estimates
    field_list = ['FID']
    ex_list = []
    for year in range(0, 16):
        for month in range(1, 13):
            if year == 0 and month == 1:
                continue
            raster_list = [os.path.join(FEWS_folder,
                                        'ea{:0>2}{:0>2}{}.bil'.format(
                                        year, month, i)) for i in range(1, 4)]            
            save_as = os.path.join(tempdir, '{}_{}.tif'.format(month, year))
            sum_rasters(raster_list, save_as, cell_size)
            field_name = 'RFE_{:0>2}_{:0>2}'.format(month, year)
            field_list.append(field_name)
            ex_list.append([save_as, field_name])
    
    # extract monthly values to each point
    arcpy.sa.ExtractMultiValuesToPoints(point_shp, ex_list)

    # read monthly values into table
    num_val = len(field_list)
    prec_dict = {'site': [], 'year': [], 'month': [], 'prec': []}
    with arcpy.da.SearchCursor(point_shp, field_list) as cursor:
        for row in cursor:
            site = row[0]
            prec_dict['site'].extend([site] * num_val)
            for f_idx in range(len(field_list)):
                field = field_list[f_idx]
                month = field[4:6]
                year = field[7:9]
                prec = row[f_idx]
                prec_dict['year'].append(year)
                prec_dict['month'].append(month)
                prec_dict['prec'].append(prec)
    prec_df = pd.DataFrame.from_dict(prec_dict)
    prec_df.to_csv(prec_table)

def generate_grass_csvs(template, input_dir):
    """Make input csvs describing grass for input to the forage model. Copy a
    template, using names taken from schedule files in the input_dir."""
    
    sch_files = [f for f in os.listdir(input_dir) if f.endswith('.sch')]
    sch_files = [f for f in sch_files if not f.endswith('hist.sch')]
    site_list = [f[:-4] for f in sch_files]
    
    template_df = pd.read_csv(template)
    for site in site_list:
        new_df = template_df.copy()
        new_df = new_df.set_value(0, 'label', site)
        save_as = os.path.join(input_dir, '{}.csv'.format(site))
        new_df.to_csv(save_as, index=False)

def generate_site_csv(input_dir, save_as):
    """Generate a csv that can be used to direct inputs to run the model."""
    
    def get_latitude(site_file):
        with open(site_file, 'r') as read_file:
            for line in read_file:
                if 'SITLAT' in line:
                    lat = line[:8].strip()
        return lat
                
    sch_files = [f for f in os.listdir(input_dir) if f.endswith('.sch')]
    sch_files = [f for f in sch_files if not f.endswith('hist.sch')]
    site_list = [f[:-4] for f in sch_files]
    
    site_dict = {'name': [], 'lat': []}
    for site in site_list:
        site_file = os.path.join(input_dir, '{}.100'.format(site))
        assert os.path.isfile(site_file), "file {} not found".format(site_file)
        lat = get_latitude(site_file)
        site_dict['name'].append(site)
        site_dict['lat'].append(lat)
    site_df = pd.DataFrame(site_dict)
    site_df.to_csv(save_as, index=False)
    
if __name__ == "__main__":
    zonal_shp = r"C:\Users\Ginger\Documents\NatCap\GIS_local\Kenya_forage\regional_properties_Jul_8_2016.shp"
    # soil_table = calc_soil_table()
    soil_table = r"C:\Users\Ginger\Desktop\Soil_avg.csv"
    # join_site_lat_long(zonal_shp, soil_table)
    save_dir = r"C:\Users\Ginger\Dropbox\NatCap_backup\Forage_model\CENTURY4.6\Kenya\input\regional_properties\Worldclim_precip"
    template_100 = r"C:\Users\Ginger\Dropbox\NatCap_backup\Forage_model\CENTURY4.6\Kenya\input\Golf_10.100"
    # write_site_files(template_100, soil_table, save_dir)
    template_hist = r"C:\Users\Ginger\Dropbox\NatCap_backup\Forage_model\CENTURY4.6\Kenya\input\regional_properties\Worldclim_precip\0_hist.sch"
    template_extend = r"C:\Users\Ginger\Dropbox\NatCap_backup\Forage_model\CENTURY4.6\Kenya\input\regional_properties\Worldclim_precip\0.sch"
    make_sch_files(template_hist, template_extend, soil_table, save_dir)
    FEWS_folder = r"C:\Users\Ginger\Documents\NatCap\GIS_local\Kenya_forage\FEWS_RFE"
    clipped_folder = r"C:\Users\Ginger\Documents\NatCap\GIS_local\Kenya_forage\FEWS_RFE_clipped"
    aoi_shp = r"C:\Users\Ginger\Documents\NatCap\GIS_local\Kenya_forage\Laikipia_soil_250m\Laikipia_soil_clip_prj.shp"
    # clip_FEWS_files(FEWS_folder, clipped_folder, aoi_shp)
    prec_table = r"C:\Users\Ginger\Desktop\prec.csv"
    # process_FEWS_files(clipped_folder, zonal_shp, prec_table)
    worldclim_temp_folder = r"C:\Users\Ginger\Documents\NatCap\GIS_local\Kenya_forage\Laikipia_Worldclim_temp"
    temp_table = r"C:\Users\Ginger\Desktop\temp.csv"
    worldclim_precip_folder = r"C:\Users\Ginger\Documents\NatCap\GIS_local\Kenya_forage\Laikipia_Worldclim_prec"
    # process_worldclim_precip(worldclim_precip_folder, zonal_shp, prec_table)
    # process_worldclim_temp(worldclim_temp_folder, zonal_shp, temp_table)
    input_folder = 'C:/Users/Ginger/Desktop/test_wth'
    # clim_tables_to_inputs(prec_table, temp_table, input_folder)
    template = r"C:\Users\Ginger\Dropbox\NatCap_backup\Forage_model\Forage_model\model_inputs\grass_suyian.csv"
    input_dir = r"C:\Users\Ginger\Dropbox\NatCap_backup\Forage_model\CENTURY4.6\Kenya\input\regional_properties\Worldclim_precip"
    # generate_grass_csvs(template, input_dir)
    site_csv = os.path.join(input_dir, 'regional_properties.csv')
    # generate_site_csv(input_dir, site_csv)
    # remove_wth_from_sch(input_dir)
    